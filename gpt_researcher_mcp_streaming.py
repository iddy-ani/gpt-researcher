#!/usr/bin/env python3
'''
MCP Server: gpt-researcher (Streaming Version)
Description: Provides AI-powered research capabilities with progress streaming

Compatible with Intel MCP Framework
'''

import asyncio
import json
import os
import sys
from datetime import datetime
from mcp.server import Server
from mcp.server.stdio import stdio_server
from mcp.types import (
    Resource,
    Tool,
    Prompt,
    TextContent,
    ImageContent,
    EmbeddedResource,
    CallToolRequest,
    ListToolsRequest,
    ListResourcesRequest,
    ReadResourceRequest,
    ListPromptsRequest,
    GetPromptRequest
)

# Add the gpt_researcher directory to the path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import GPT Researcher
try:
    from gpt_researcher import GPTResearcher
    from gpt_researcher.config.config import Config
except ImportError as e:
    print(f"Error importing GPT Researcher: {e}", file=sys.stderr)
    sys.exit(1)

# Initialize the MCP server
server = Server("gpt-researcher")

# Global reference to write stream for notifications
_write_stream = None

def send_progress_notification(message: str, progress: float = None):
    """Send a progress notification to the client"""
    global _write_stream
    # For now, disable progress notifications in executable to prevent crashes
    # Just log progress to stderr for debugging
    try:
        if progress is not None:
            print(f"Progress: {message} ({progress:.1%})", file=sys.stderr)
        else:
            print(f"Progress: {message}", file=sys.stderr)
    except Exception as e:
        # Silently continue if even stderr fails
        pass

# Report types supported by GPT Researcher
SUPPORTED_REPORT_TYPES = [
    "research_report",
    "custom_report", 
    "subtopic_report",
    "outline_report",
]

async def conduct_research_task(arguments: dict) -> list[dict]:
    '''
    Conduct comprehensive research with progress updates
    '''
    query = arguments.get('query', '').strip()
    report_type = arguments.get('report_type', 'research_report')
    
    if not query:
        return [{
            "type": "text",
            "text": "Error: Query is required. Please provide a research topic or question."
        }]
    
    if report_type not in SUPPORTED_REPORT_TYPES:
        report_type = "research_report"
    
    try:
        send_progress_notification(f"ğŸ” Starting research on: {query}", 0.1)
        
        # Initialize GPT Researcher
        researcher = GPTResearcher(query=query, report_type=report_type)
        
        send_progress_notification("âš™ï¸ Configuring research parameters...", 0.2)
        
        # Conduct research with progress updates
        send_progress_notification("ğŸŒ Conducting web research...", 0.3)
        research_result = await researcher.conduct_research()
        
        send_progress_notification("ğŸ“ Analyzing findings and generating report...", 0.7)
        
        # Generate report
        report = await researcher.write_report()
        
        send_progress_notification("âœ… Research completed successfully!", 1.0)
        
        # Format the response
        response_text = f"""# Research Report: {query}

**Report Type:** {report_type}
**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Sources Found:** {len(research_result) if research_result else 0}

---

{report}

---

*Report generated by GPT Researcher using Intel's internal API*
"""
        
        return [{
            "type": "text",
            "text": response_text
        }]
        
    except Exception as e:
        error_msg = f"Research failed: {str(e)}"
        send_progress_notification(f"âŒ {error_msg}", 0.0)
        print(f"âŒ {error_msg}", file=sys.stderr)
        return [{
            "type": "text",
            "text": f"Error: {error_msg}"
        }]

async def quick_research(arguments: dict) -> list[dict]:
    '''
    Conduct quick research with progress updates
    '''
    query = arguments.get('query', '').strip()
    
    if not query:
        return [{
            "type": "text",
            "text": "Error: Query is required. Please provide a research topic or question."
        }]
    
    try:
        send_progress_notification(f"âš¡ Starting quick research: {query}", 0.1)
        
        # Initialize GPT Researcher with custom settings for quick research
        researcher = GPTResearcher(query=query, report_type="research_report")
        
        # Override some config for faster results
        config = researcher.cfg
        config.max_iterations = 2  # Fewer iterations
        config.max_search_results_per_query = 3  # Fewer sources per query
        
        send_progress_notification("ğŸ” Gathering initial sources...", 0.4)
        
        # Conduct research
        research_result = await researcher.conduct_research()
        
        send_progress_notification("ğŸ“„ Generating quick report...", 0.8)
        
        # Generate report
        report = await researcher.write_report()
        
        send_progress_notification("âœ… Quick research completed!", 1.0)
        
        # Format the response
        response_text = f"""# Quick Research: {query}

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Sources:** {len(research_result) if research_result else 0}

{report}

---

*Quick research by GPT Researcher using Intel's internal API*
"""
        
        return [{
            "type": "text",
            "text": response_text
        }]
        
    except Exception as e:
        error_msg = f"Quick research failed: {str(e)}"
        send_progress_notification(f"âŒ {error_msg}", 0.0)
        print(f"âŒ {error_msg}", file=sys.stderr)
        return [{
            "type": "text",
            "text": f"Error: {error_msg}"
        }]

async def generate_subtopics(arguments: dict) -> list[dict]:
    '''
    Generate subtopics for a research area
    '''
    query = arguments.get('query', '').strip()
    max_subtopics = arguments.get('max_subtopics', 5)
    
    if not query:
        return [{
            "type": "text",
            "text": "Error: Query is required. Please provide a main research topic."
        }]
    
    try:
        send_progress_notification(f"ğŸ§  Generating subtopics for: {query}", 0.3)
        
        # Initialize researcher for subtopic generation
        researcher = GPTResearcher(query=query, report_type="subtopic_report")
        
        send_progress_notification("ğŸ” Analyzing topic structure...", 0.7)
        
        # Use GPT Researcher's built-in subtopic generation
        from gpt_researcher.utils.llm import construct_subtopics
        
        subtopics_response = await construct_subtopics(
            task=query,
            data="",  # No prior context
            config=researcher.cfg,
            subtopics=max_subtopics,
            prompt_family=None,
        )
        
        send_progress_notification("âœ… Subtopics generated successfully!", 1.0)
        
        response_text = f"""# Subtopics for: {query}

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Number of Subtopics:** {max_subtopics}

{subtopics_response}

---

*Subtopics generated by GPT Researcher using Intel's internal API*
"""
        
        return [{
            "type": "text",
            "text": response_text
        }]
        
    except Exception as e:
        error_msg = f"Subtopic generation failed: {str(e)}"
        send_progress_notification(f"âŒ {error_msg}", 0.0)
        print(f"âŒ {error_msg}", file=sys.stderr)
        return [{
            "type": "text",
            "text": f"Error: {error_msg}"
        }]

async def check_system_status(arguments: dict) -> list[dict]:
    '''
    Check GPT Researcher system status and configuration
    '''
    try:
        send_progress_notification("ğŸ”§ Checking system configuration...", 0.5)
        
        config = Config()
        
        # Basic configuration check
        status_info = {
            "system_status": "âœ… Operational",
            "llm_provider": getattr(config, 'smart_llm_provider', 'Unknown'),
            "llm_model": getattr(config, 'smart_llm_model', 'Unknown'),
            "api_base": getattr(config, 'openai_api_base', getattr(config, 'smart_llm_api_base', 'Unknown')),
            "retrievers": getattr(config, 'retrievers', ['Unknown']),
            "max_search_results": getattr(config, 'max_search_results_per_query', 'Unknown'),
            "max_iterations": getattr(config, 'max_iterations', 'Unknown'),
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        send_progress_notification("âœ… System status check completed!", 1.0)
        
        status_text = f"""# GPT Researcher System Status

**Status:** {status_info['system_status']}
**Timestamp:** {status_info['timestamp']}

## Configuration
- **LLM Provider:** {status_info['llm_provider']}
- **LLM Model:** {status_info['llm_model']}
- **API Base:** {status_info['api_base']}
- **Retrievers:** {', '.join(status_info['retrievers']) if isinstance(status_info['retrievers'], list) else status_info['retrievers']}
- **Max Search Results:** {status_info['max_search_results']}
- **Max Iterations:** {status_info['max_iterations']}

## Available Tools
- conduct-research: Comprehensive AI-powered research
- quick-research: Fast research with fewer sources
- generate-subtopics: Generate subtopics for research areas
- check-status: System status and configuration check

---

*Status check by GPT Researcher MCP Server*
"""
        
        return [{
            "type": "text",
            "text": status_text
        }]
        
    except Exception as e:
        error_msg = f"Status check failed: {str(e)}"
        send_progress_notification(f"âŒ {error_msg}", 0.0)
        print(f"âŒ {error_msg}", file=sys.stderr)
        return [{
            "type": "text",
            "text": f"Error: {error_msg}"
        }]

# MCP Server Implementation
@server.list_tools()
async def handle_list_tools() -> list[Tool]:
    """List available tools."""
    return [
        Tool(
            name="conduct-research",
            description="Conduct comprehensive AI-powered research on any topic with progress updates",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Research topic or question"
                    },
                    "report_type": {
                        "type": "string",
                        "enum": SUPPORTED_REPORT_TYPES,
                        "default": "research_report",
                        "description": "Type of report to generate"
                    }
                },
                "required": ["query"]
            }
        ),
        Tool(
            name="quick-research",
            description="Conduct quick research with fewer sources for faster results (with progress updates)",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Research topic or question"
                    }
                },
                "required": ["query"]
            }
        ),
        Tool(
            name="generate-subtopics",
            description="Generate subtopics for a research area",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Main research topic"
                    },
                    "max_subtopics": {
                        "type": "integer",
                        "minimum": 3,
                        "maximum": 10,
                        "default": 5,
                        "description": "Maximum number of subtopics to generate"
                    }
                },
                "required": ["query"]
            }
        ),
        Tool(
            name="check-status",
            description="Check GPT Researcher system status and configuration",
            inputSchema={
                "type": "object",
                "properties": {},
                "required": []
            }
        )
    ]

@server.call_tool()
async def handle_call_tool(name: str, arguments: dict) -> list[dict]:
    """Handle tool calls."""
    if name == "conduct-research":
        return await conduct_research_task(arguments)
    elif name == "quick-research":
        return await quick_research(arguments)
    elif name == "generate-subtopics":
        return await generate_subtopics(arguments)
    elif name == "check-status":
        return await check_system_status(arguments)
    else:
        raise ValueError(f"Unknown tool: {name}")

async def main():
    """Main function to run the MCP server"""
    global _write_stream
    
    try:
        # Ensure UTF-8 encoding for stdout/stderr
        if hasattr(sys.stdout, 'reconfigure'):
            sys.stdout.reconfigure(encoding='utf-8')
            sys.stderr.reconfigure(encoding='utf-8')
        
        print("ğŸš€ Starting GPT Researcher MCP Server (Streaming)...", file=sys.stderr)
        
        # Check basic configuration
        config = Config()
        print(f"ğŸ“Š LLM Provider: {config.smart_llm_provider}", file=sys.stderr)
        print(f"ğŸ” Retrievers: {', '.join(config.retrievers) if hasattr(config, 'retrievers') else 'Unknown'}", file=sys.stderr)
        
        async with stdio_server() as (read_stream, write_stream):
            # Store write stream globally for notifications
            _write_stream = write_stream
            await server.run(read_stream, write_stream, server.create_initialization_options())
            
    except Exception as e:
        print(f"âŒ Failed to start MCP server: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())